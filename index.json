[{"content":"Slopaganda: When AI Becomes a Weapon for Your Mind Here\u0026rsquo;s what should terrify you: News Corp Australia churns out 3,000 AI-generated \u0026ldquo;local\u0026rdquo; news stories every week. Not written by journalists. Generated by algorithms. And they\u0026rsquo;re getting better at lying than humans are.\nLet me think about this for a moment. We\u0026rsquo;ve always had propaganda. Goebbels had his radio broadcasts. Napoleon had his pamphlets. The Catholic Church had its missionaries. But something fundamentally different is happening now, and three researchers just mapped out exactly how screwed we might be.\nThey Call It \u0026ldquo;Slopaganda.\u0026rdquo; Here\u0026rsquo;s What That Really Means. The term is newâ€”a mashup of \u0026ldquo;slop\u0026rdquo; (unwanted AI-generated garbage) and \u0026ldquo;propaganda\u0026rdquo; (intentional belief manipulation for political ends). But don\u0026rsquo;t let the silly name fool you. This is the difference between a knife and a nuclear weapon.\nTraditional propaganda had limits. You needed printing presses, radio stations, TV networks. You broadcast the same message to everyone. It was expensive. It was slow. And sophisticated people could spot it.\nSlopaganda? It knows you. It knows what time you\u0026rsquo;re awake. It knows your zip code, your commute length, which Facebook pages you liked five years ago. It can generate personalized messages faster than you can read them. And here\u0026rsquo;s the beautiful, terrible thing: it doesn\u0026rsquo;t just lie. It tells you exactly what you already half-believe, wrapped in language that feels local, familiar, true.\nYour Brain Is Playing Checkers. Slopaganda Is Playing 4D Chess. Let\u0026rsquo;s talk about how your brain actually makes decisions, because the paper\u0026rsquo;s authorsâ€”cognitive scientists who study this stuffâ€”lay it out with surgical precision.\nDecision-making isn\u0026rsquo;t some magical moment of free will. It\u0026rsquo;s a multi-stage process that uses accumulated information at every step. When that information is correct, you make good choices and feel a strong sense of agency. When it\u0026rsquo;s wrong? You make bad decisions and don\u0026rsquo;t even realize why.\nNow here\u0026rsquo;s where it gets interesting. Your brain has gatekeepersâ€”attention and memory systems that decide what information matters enough to store. And these systems are hilariously easy to hijack.\nThe Attention Economy Problem: You\u0026rsquo;re drowning in information. There\u0026rsquo;s no possible way to process it all. So your brain has a simple rule: pay attention to threats. Makes sense evolutionarilyâ€”better to have a thousand false alarms than miss one tiger.\nSlopaganda exploits this mercilessly. Feed people threatening content targeted to their specific fears, and boomâ€”it captures attention. Gets encoded in memory. Influences decisions for years.\nThe Emotional Amplifier: Your brain remembers negative information better. It\u0026rsquo;s called negativity bias. You remember insults more vividly than compliments. Threats more than comforts. Violence more than peace.\nAI can now generate an endless stream of emotionally-charged, threatening messages customized to your psychological profile. Worried about justice? Here\u0026rsquo;s AI-generated content about injustice in your area. Concerned about harm? Here are fabricated stories about violence near you.\nThe Confirmation Trap: Here\u0026rsquo;s the really nasty part. When you encounter information that conflicts with your beliefs, you reject it with negative emotion. When you encounter information that confirms what you already think? Your brain lights up like a Christmas tree. \u0026ldquo;Yes! I knew it!\u0026rdquo;\nResearchers tested this. They can now microtarget content that reinforces your existing biases while hiding alternative perspectives. It\u0026rsquo;s not even that hard.\nThe Continued Influence Effect (Or: Why Correcting Lies Doesn\u0026rsquo;t Work) You know what\u0026rsquo;s genuinely disturbing? When researchers correct false information in people\u0026rsquo;s brains, traces of the original lie persist. Like trying to delete text from a documentâ€”the words disappear, but the indent remains.\nPicture this: You read an AI-generated story claiming a politician took bribes. Later, you learn it was false. But your brain still has that politician filed under \u0026ldquo;corrupt.\u0026rdquo; The correction doesn\u0026rsquo;t erase the neural pathway. It just adds a competing one.\nNow multiply this by thousands of personalized messages per day, targeted to millions of people. Even if fact-checkers catch 90% of it (they won\u0026rsquo;t), the remaining 10% leaves permanent traces. Small effects at scale swing elections. Undermine public health. Start wars.\nThree Ways Slopaganda Is Different From Everything Before Scale: AI produces content faster than any human or content farm ever could. Orders of magnitude faster.\nScope: Personalization at the individual level. Not just \u0026ldquo;local news\u0026rdquo; but \u0026ldquo;news calibrated to your psychological profile, delivered when you\u0026rsquo;re most vulnerable.\u0026rdquo;\nSpeed: Hyper-connected social networks plus algorithmic amplification. A lie circles the globe before truth puts on its shoes.\nThe paper\u0026rsquo;s authors invoke \u0026ldquo;Brandolini\u0026rsquo;s Law\u0026rdquo;â€”it takes ten times more energy to refute bullshit than to produce it. Slopaganda just made that ratio 100 to 1. Maybe 1,000 to 1.\nReal Examples That Should Worry You Sinclair Media owned dozens of \u0026ldquo;local\u0026rdquo; news stations across America. One day, someone noticed all these supposedly independent local anchors were reading the exact same script, word for word. \u0026ldquo;This is extremely dangerous to our democracy,\u0026rdquo; they all intoned, identically.\nThat was 2018. Pre-AI. Now imagine that but personalized, generated at scale, impossible to trace back to a single script.\nSteve Bannon, founder of Breitbart News, bragged his strategy was to \u0026ldquo;flood the zone with shit.\u0026rdquo; Researchers found Breitbart had a semantic tag called \u0026ldquo;Black Crime\u0026rdquo; (no corresponding \u0026ldquo;White Crime\u0026rdquo; tag, naturally). Half a dozen sensationalistic stories designed to create racial associations in readers\u0026rsquo; brains.\nThat was before generative AI. Imagine flooding the zone with a million pieces of racially-charged shit per day, each one calibrated to its reader\u0026rsquo;s existing prejudices.\nDuring the 2024 US election, Trump posted AI-generated images suggesting Taylor Swift endorsed him. Elon Musk posted deepfake audio of Kamala Harris saying embarrassing things she never said. These were one-offs. The real danger is when this becomes industrialized, automated, personalized.\nThe Illusory Truth Effect (Or: How Repetition Becomes Reality) Here\u0026rsquo;s a psychological phenomenon that makes all of this worse: if you hear something repeated enough times, your brain starts treating it as true. Doesn\u0026rsquo;t matter if it\u0026rsquo;s false. Familiarity becomes a cue for truthfulness.\nGoebbels supposedly said, \u0026ldquo;Repeat a lie often enough, and it becomes the truth.\u0026rdquo; The researchers note this is probably apocryphalâ€”but the principle is scientifically validated.\nNow give that principle to AI that can generate personalized content at infinite scale and speed. You\u0026rsquo;ll see the same narrative surface repeatedly in your feed, from \u0026ldquo;different\u0026rdquo; sources, all generated by the same system. Your brain never stood a chance.\nWhat Can We Do? (Spoiler: Nothing Easy) The paper\u0026rsquo;s authors are honest about how hard this problem is. They suggest:\nPsychological interventions: Games like \u0026ldquo;Bad News\u0026rdquo; that let people practice spotting manipulation tactics. Digital literacy education. Journaling exercises to build intellectual humility. These work. Small effects. High cost. Not enough.\nTechnological solutions: AI detection tools. Content moderation systems. Browser plugins that encourage self-reflection. Better recommendation algorithms. The challenge? Every defense creates an arms race. The offense has more resources.\nEconomic and political interventions: Here\u0026rsquo;s where the authors get bold. They point out that oligarchs with more money than they could ever spend are using their wealth to pollute the information ecosystem. Their suggestion? A global wealth tax to reduce oligarchic power and fund counter-measures.\nRather remarkable, actuallyâ€”academic researchers proposing wealth redistribution as a solution to an epistemology problem. But they\u0026rsquo;re not wrong. The problem isn\u0026rsquo;t just cognitive. It\u0026rsquo;s material. It\u0026rsquo;s about who has power.\nThe Beautiful, Terrible Truth Slopaganda represents a phase change in how power operates. It\u0026rsquo;s not just propaganda getting better. It\u0026rsquo;s a qualitatively new phenomenonâ€”targeting meets personalization meets scale meets speed meets psychological sophistication.\nSmall effects at large scale, delivered with precision to people in their moments of vulnerability, accumulating as neural traces that persist even after correctionâ€”this is the epistemic environment we now inhabit.\nThe authors conclude with something I find both inspiring and depressing: we need to be bold enough to propose solutions that seem impossible, because the status quo is untenable. \u0026ldquo;Infeasibility is often cynically used as an objection by those who benefit unjustly from the status quo.\u0026rdquo;\nIn other words: yeah, global wealth taxes seem impossible. So does living in a world where truth matters. Pick your impossibility.\nThe Part They Don\u0026rsquo;t Want You To Know Sample sizes matter. Funding sources matter. Replication matters. This paper is a conceptual analysis with literature reviewâ€”not an experimental study. The authors are synthesizing existing research on cognitive science, AI capabilities, and propaganda to argue that slopaganda is a unique threat.\nAre they right? The evidence they marshal is compelling. The mechanisms they describe are well-established. But we\u0026rsquo;re still in early days. We don\u0026rsquo;t yet have longitudinal studies showing slopaganda\u0026rsquo;s long-term effects.\nWhat we do have: AI systems that are provably better at producing persuasive disinformation than humans. Platforms deploying these systems at scale. Evidence that microtargeting works. And a political economy that rewards flooding zones with shit.\nThat\u0026rsquo;s not proof. But it\u0026rsquo;s a rather concerning convergence of factors, actually.\nRead the Original Slopaganda: The interaction between propaganda and generative AI by MichaÅ‚ Klincewicz, Mark Alfano, and Amir Ebrahimi Fard (2025)\nðŸ“„ Read it here: https://arxiv.org/pdf/2503.01560.pdf\nThe beautiful thing about science is that it gives us tools to understand threats before they destroy us. The terrible thing is we have to choose to use those tools. Your move.\n","permalink":"https://otechai.github.io/posts/2503.01560/","summary":"\u003ch1 id=\"slopaganda-when-ai-becomes-a-weapon-for-your-mind\"\u003eSlopaganda: When AI Becomes a Weapon for Your Mind\u003c/h1\u003e\n\u003cp\u003eHere\u0026rsquo;s what should terrify you: News Corp Australia churns out 3,000 AI-generated \u0026ldquo;local\u0026rdquo; news stories every week. Not written by journalists. Generated by algorithms. And they\u0026rsquo;re getting better at lying than humans are.\u003c/p\u003e\n\u003cp\u003eLet me think about this for a moment. We\u0026rsquo;ve always had propaganda. Goebbels had his radio broadcasts. Napoleon had his pamphlets. The Catholic Church had its missionaries. But something fundamentally different is happening now, and three researchers just mapped out exactly how screwed we might be.\u003c/p\u003e","title":"Slopaganda: When AI Becomes a Weapon for Your Mind"}]